{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15b5e64a-7867-42fa-bb9d-71f8c5cad897",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Linear Algeabra is not basic algeabra\n",
    "\n",
    "# Linear algebra is a branch of mathematics that deals with vector spaces and linear equations\n",
    "\n",
    "# keywords: Vectors, Matrices , Linear Equations, Vector Spaces, Linear Transformations, Eigenvalues and Eigenvectors, determinant\n",
    "\n",
    "# As a data scientist learn to remember concepts as a CS student( as programming like arrays , matrix) and also with visual understanding\n",
    "\n",
    "# This notebook is purely for people who are already familiar with concepts of vector or linear algeabra ....if not read the textbook (essential math for data science)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9727b990-a3c3-49ea-8a5e-1fd44bd6cb37",
   "metadata": {},
   "outputs": [],
   "source": [
    "# some of the key findings or ideas from vector algeabra\n",
    "\n",
    "# 1. Always imagine a vector tail starts at the origin of cartesian plane(0,0)\n",
    "# 2. Python is slow whereas its numeric and scientific libraries are not slow eg:numpy ( which we are going to use for linear algeabra )\n",
    "# 3. each vector can be represented as group of hat vectors also called basis vectors moving in a certain direction\n",
    "# 4. operations: scaling, shear, rotate\n",
    "# 5. when we have two vectors in two different direction they are linearly independent and have this unlimited span \n",
    "# 6. when the combination of vectors is stuck on same line then they just limit our span to the same line and this is called linealy dependent.\n",
    "# 7. why check for linear dependence : A lot of problems become difficult or unsolvable when they are linearly dependent \n",
    "# for eg: when we are dealing with system of equations, a linearly dependent set of equations can cause variables to disappear and make the problem unsolvable\n",
    "# and we use determinant to check for linear dependence for a group of vectors or system of equations\n",
    "# 8. The flexibility to create any vector you need from two or more vectors becomes invaluable to solve for a solution\n",
    "# 9. Always use the goal is create some new vector or matrix\n",
    "# 10. You create new vector by performing some transformations on some vectors \n",
    "# 11. Basis vectors acts a ground level transformation for whatever the vector.. so let's call it a sequence of transformations ( like scale,shear,rotate) until you get your final vector or matrix starting with a single vector\n",
    "# 12. You can't have transformations that are non linear, resulting in curvy or squiggly transformation ..remember it is linear algeabra not non linear algeabra\n",
    "# 13. If you want true vector algeabra enlightment, think why creating vectors and transforming vectors are actually the same thing.\n",
    "# It's all a matter of relativity given your basis vectors being a starting point (the most basic starting point i_hat and j_hat) before and after transformation.\n",
    "# 14. A matrix is really is a transformation expressed as basis vectors\n",
    "# 15. Non square matrix can transform vector space into fewer or more dimensions\n",
    "# 16. See matrix multiplication as pulling basis vectors or transformation in a certain way- updated basis* old basis\n",
    "# 17. A determinant measures how a linear transformation scales an area\n",
    "# 18. A simple shear or rotate should not affect the determinant as the area will not change which is usually 1 and it is negative when the orientation flips and scaling will increase or decrease the determinant\n",
    "# 19. If you have a determinant of 0 that means all of the space has been squished into a lesser dimension which is called linear dependence\n",
    "# 20. Square matrix -- represent linear transformations and a requirement in eigen decomposition\n",
    "# Identity matrix -- you have undone a transformation and found your starting basis vectors\n",
    "# Inverse matrix -- It will undoes the transformation of another matrix\n",
    "# Diagonal matrix -- Desirable in certain computations because they represent simple scalars being applied to a vector space\n",
    "# Traingular matrix -- Easier to solve systems of equations\n",
    "# sparse matrix -- To create efficiency by not wasting space storing a bunch of 0's and 1's instead only keep track of the cells that are non zero\n",
    "# Eigen decomposition is helpful for breaking up matrix into components that are easier to work with\n",
    "# Eigen vector is literally the vector who has a scalar change but not anything when you do the transformation and that eigen value is the scalar value by which it changed\n",
    "# There is one eigen vector and eigen value for each dimension of the parent matrix and not alll matrices can be decomposed into an eigen vector and eigen value sometimes complex numbers will even result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "bd298a2e-be61-4ecf-8314-c5cdf9e59061",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[5 3]\n"
     ]
    }
   ],
   "source": [
    "# Adding two vectors\n",
    "\n",
    "from numpy import array\n",
    "\n",
    "v = array([3,2])\n",
    "w = array([2,1])\n",
    "s=v+w\n",
    "print(s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "743bc859-e23b-407e-9d6b-9fe3dd09fa2e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[6. 4.]\n"
     ]
    }
   ],
   "source": [
    "# Scaling vectors\n",
    "\n",
    "scaled_v= 2.0 * v\n",
    "print(scaled_v)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "16b3234d-3b3c-469c-9ed3-a53e46d4bb6b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[3 2]\n"
     ]
    }
   ],
   "source": [
    "# Matrix multiplication\n",
    "\n",
    "basis= array([[3,0],[0,2]])\n",
    "v= array([1,1])\n",
    "new_v= basis.dot(v)\n",
    "\n",
    "print(new_v)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2acdeccb-c72d-4c50-a842-adbf1c33529a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# if you break out the basis vectors and then compose them together into a matrix..just note you need to transpose or swap the columns and rows\n",
    "# because numpy's array function will consider each vector as  a row rather than a column\n",
    "# array([1,1],[2,2]) is not same as x = [1,1] and y= [2,2] and array([x,y]) instead it is array([x,y]).transpose()\n",
    "# use matmul or @ operator to perform matrix multiplication\n",
    "#To apply these transformations in sequence (shear, then rotate, then scale), we multiply the matrices in reverse order of application: T=S with scale⋅R⋅S with shear "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a6b6326c-db16-4d90-8828-cd989c3281a3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shear Matrix:\n",
      " [[1.  1. ]\n",
      " [0.5 1. ]]\n",
      "Rotation Matrix:\n",
      " [[ 0.8660254 -0.5      ]\n",
      " [ 0.5        0.8660254]]\n",
      "Scale Matrix:\n",
      " [[2.  0. ]\n",
      " [0.  1.5]]\n",
      "Combined Transformation Matrix:\n",
      " [[1.23205081 0.73205081]\n",
      " [1.39951905 2.04903811]]\n",
      "Original point: [1 1]\n",
      "Transformed point: [1.96410162 3.44855716]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Define the transformation parameters\n",
    "kx, ky = 1.0, 0.5  # Shear factors\n",
    "theta = np.radians(30)  # Rotation angle in radians\n",
    "sx, sy = 2.0, 1.5  # Scale factors\n",
    "\n",
    "# Shear matrix\n",
    "shear_matrix = np.array([\n",
    "    [1, kx],\n",
    "    [ky, 1]\n",
    "])\n",
    "\n",
    "# Rotation matrix\n",
    "rotation_matrix = np.array([\n",
    "    [np.cos(theta), -np.sin(theta)],\n",
    "    [np.sin(theta), np.cos(theta)]\n",
    "])\n",
    "\n",
    "# Scale matrix\n",
    "scale_matrix = np.array([\n",
    "    [sx, 0],\n",
    "    [0, sy]\n",
    "])\n",
    "\n",
    "# Combine the transformations: Scale -> Rotate -> Shear\n",
    "combined_matrix = scale_matrix @ rotation_matrix @ shear_matrix\n",
    "\n",
    "# Define a point to transform\n",
    "point = np.array([1, 1])\n",
    "\n",
    "# Apply the combined transformation\n",
    "transformed_point = combined_matrix @ point\n",
    "\n",
    "# Print the results\n",
    "print(\"Shear Matrix:\\n\", shear_matrix)\n",
    "print(\"Rotation Matrix:\\n\", rotation_matrix)\n",
    "print(\"Scale Matrix:\\n\", scale_matrix)\n",
    "print(\"Combined Transformation Matrix:\\n\", combined_matrix)\n",
    "print(\"Original point:\", point)\n",
    "print(\"Transformed point:\", transformed_point)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b0e7d0ef-936b-46de-bebe-4aa8cb938191",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Coefficient Matrix (A):\n",
      "[[2 3]\n",
      " [5 4]]\n",
      "\n",
      "Constant Matrix (B):\n",
      "[[ 8]\n",
      " [14]]\n",
      "\n",
      "Inverse of Matrix (A):\n",
      "[[-0.57142857  0.42857143]\n",
      " [ 0.71428571 -0.28571429]]\n",
      "\n",
      "Solution (X):\n",
      "[[1.42857143]\n",
      " [1.71428571]]\n"
     ]
    }
   ],
   "source": [
    "# solving a system of equations using inverse\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "# Define the coefficient matrix A and the constant matrix B\n",
    "A = np.array([[2, 3], [5, 4]])\n",
    "B = np.array([[8], [14]])\n",
    "\n",
    "# Calculate the inverse of A\n",
    "A_inv = np.linalg.inv(A)\n",
    "\n",
    "# Solve for X\n",
    "X = np.dot(A_inv, B)\n",
    "\n",
    "# Print the results\n",
    "print(\"Coefficient Matrix (A):\")\n",
    "print(A)\n",
    "print(\"\\nConstant Matrix (B):\")\n",
    "print(B)\n",
    "print(\"\\nInverse of Matrix (A):\")\n",
    "print(A_inv)\n",
    "print(\"\\nSolution (X):\")\n",
    "print(X)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "48eb17b5-68d2-4524-a00b-aaa76e15cc9d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original matrix (A):\n",
      "[[ 4 -2]\n",
      " [ 1  1]]\n",
      "\n",
      "Eigenvalues:\n",
      "[3. 2.]\n",
      "\n",
      "Eigenvectors (V):\n",
      "[[0.89442719 0.70710678]\n",
      " [0.4472136  0.70710678]]\n",
      "\n",
      "Diagonal matrix of eigenvalues (Lambda):\n",
      "[[3. 0.]\n",
      " [0. 2.]]\n",
      "\n",
      "Inverse of the eigenvectors matrix (V^-1):\n",
      "[[ 2.23606798 -2.23606798]\n",
      " [-1.41421356  2.82842712]]\n",
      "\n",
      "Reconstructed matrix (A_reconstructed):\n",
      "[[ 4. -2.]\n",
      " [ 1.  1.]]\n"
     ]
    }
   ],
   "source": [
    "# Constructing original matrix from eigen values and eigen vectors\n",
    "\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "# Original matrix\n",
    "A = np.array([[4, -2],\n",
    "              [1,  1]])\n",
    "\n",
    "# Compute eigenvalues and eigenvectors\n",
    "eigenvalues, eigenvectors = np.linalg.eig(A)\n",
    "\n",
    "# Construct the diagonal matrix of eigenvalues\n",
    "Lambda = np.diag(eigenvalues)\n",
    "\n",
    "# Eigenvectors matrix (V)\n",
    "V = eigenvectors\n",
    "\n",
    "# Inverse of the eigenvectors matrix (V^-1)\n",
    "V_inv = np.linalg.inv(V)\n",
    "\n",
    "# Reconstruct the original matrix\n",
    "A_reconstructed = V @ Lambda @ V_inv\n",
    "\n",
    "# Print the results\n",
    "print(\"Original matrix (A):\")\n",
    "print(A)\n",
    "print(\"\\nEigenvalues:\")\n",
    "print(eigenvalues)\n",
    "print(\"\\nEigenvectors (V):\")\n",
    "print(V)\n",
    "print(\"\\nDiagonal matrix of eigenvalues (Lambda):\")\n",
    "print(Lambda)\n",
    "print(\"\\nInverse of the eigenvectors matrix (V^-1):\")\n",
    "print(V_inv)\n",
    "print(\"\\nReconstructed matrix (A_reconstructed):\")\n",
    "print(A_reconstructed)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c63c316b-263b-4b15-856a-f6cca9d718a5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
